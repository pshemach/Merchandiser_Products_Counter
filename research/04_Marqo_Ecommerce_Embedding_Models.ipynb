{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4b8eacad",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\HP\\Desktop\\E-Vision-Projects\\Merchandiser_Products_Counter\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoProcessor, AutoModel\n",
    "import torch\n",
    "import numpy as np\n",
    "import faiss\n",
    "from PIL import Image\n",
    "import os\n",
    "import glob\n",
    "\n",
    "import open_clip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4055cc78",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\HP\\Desktop\\E-Vision-Projects\\Merchandiser_Products_Counter\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching 1 files: 100%|██████████| 1/1 [00:00<?, ?it/s]\n",
      "Fetching 1 files: 100%|██████████| 1/1 [00:00<?, ?it/s]\n",
      "You are using a model of type siglip to instantiate a model of type . This is not supported for all configurations of models and can yield errors.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error loading model: Cannot copy out of meta tensor; no data! Please use torch.nn.Module.to_empty() instead of torch.nn.Module.to() when moving module from meta to a different device.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching 1 files: 100%|██████████| 1/1 [00:01<00:00,  1.24s/it]\n",
      "Fetching 1 files: 100%|██████████| 1/1 [00:00<00:00,  1.18it/s]\n",
      "You are using a model of type siglip to instantiate a model of type . This is not supported for all configurations of models and can yield errors.\n",
      "Error while downloading from https://cdn-lfs-us-1.hf.co/repos/2b/ba/2bba274ddab7c3d74e51a70aa38841cffc67797619081eb47c9b29b09c0ea4a1/5f54e3323fc98caddba9626aa9771efd873c3cb9d63cc65b4619c2ccb6213e4e?response-content-disposition=inline%3B+filename*%3DUTF-8%27%27model.safetensors%3B+filename%3D%22model.safetensors%22%3B&Expires=1757436317&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTc1NzQzNjMxN319LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy11cy0xLmhmLmNvL3JlcG9zLzJiL2JhLzJiYmEyNzRkZGFiN2MzZDc0ZTUxYTcwYWEzODg0MWNmZmM2Nzc5NzYxOTA4MWViNDdjOWIyOWIwOWMwZWE0YTEvNWY1NGUzMzIzZmM5OGNhZGRiYTk2MjZhYTk3NzFlZmQ4NzNjM2NiOWQ2M2NjNjViNDYxOWMyY2NiNjIxM2U0ZT9yZXNwb25zZS1jb250ZW50LWRpc3Bvc2l0aW9uPSoifV19&Signature=MXVEa%7EIcqBX-Nt93d42WbQSQ3najLzw90HC8r6aJi%7ENSaFNdDYEeNm4xoaglEDYEZ64EgtrgNKtNBYv%7EfqUNDa8UyEwj%7EQaMSwQkPyy3-CYFWk169NcpaGXtj85l3XrQmKqtoW7Na4fTDD%7Ezy209CSp6In4FxX3bEYn-Utx8rQE7IQFF0DasPOEDTUG5tjfsWRcanhYCUTl9aVhd9C31kxqAJxOL8StZW0ztEwkvLWUn-fJXDyxId4zfu-pbfFAct6r2lMn3DkjFjXIXyFN5VqBIW-Nt6b0Czik30R6qsEDXovIDdFLoRirCZ6ZaCUaCblGfrVfVcjW2JT15QvcDyA__&Key-Pair-Id=K24J24Z295AEI9: HTTPSConnectionPool(host='cdn-lfs-us-1.hf.co', port=443): Read timed out.\n",
      "Trying to resume download...\n",
      "WARNING:huggingface_hub.file_download:Error while downloading from https://cdn-lfs-us-1.hf.co/repos/2b/ba/2bba274ddab7c3d74e51a70aa38841cffc67797619081eb47c9b29b09c0ea4a1/5f54e3323fc98caddba9626aa9771efd873c3cb9d63cc65b4619c2ccb6213e4e?response-content-disposition=inline%3B+filename*%3DUTF-8%27%27model.safetensors%3B+filename%3D%22model.safetensors%22%3B&Expires=1757436317&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTc1NzQzNjMxN319LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy11cy0xLmhmLmNvL3JlcG9zLzJiL2JhLzJiYmEyNzRkZGFiN2MzZDc0ZTUxYTcwYWEzODg0MWNmZmM2Nzc5NzYxOTA4MWViNDdjOWIyOWIwOWMwZWE0YTEvNWY1NGUzMzIzZmM5OGNhZGRiYTk2MjZhYTk3NzFlZmQ4NzNjM2NiOWQ2M2NjNjViNDYxOWMyY2NiNjIxM2U0ZT9yZXNwb25zZS1jb250ZW50LWRpc3Bvc2l0aW9uPSoifV19&Signature=MXVEa%7EIcqBX-Nt93d42WbQSQ3najLzw90HC8r6aJi%7ENSaFNdDYEeNm4xoaglEDYEZ64EgtrgNKtNBYv%7EfqUNDa8UyEwj%7EQaMSwQkPyy3-CYFWk169NcpaGXtj85l3XrQmKqtoW7Na4fTDD%7Ezy209CSp6In4FxX3bEYn-Utx8rQE7IQFF0DasPOEDTUG5tjfsWRcanhYCUTl9aVhd9C31kxqAJxOL8StZW0ztEwkvLWUn-fJXDyxId4zfu-pbfFAct6r2lMn3DkjFjXIXyFN5VqBIW-Nt6b0Czik30R6qsEDXovIDdFLoRirCZ6ZaCUaCblGfrVfVcjW2JT15QvcDyA__&Key-Pair-Id=K24J24Z295AEI9: HTTPSConnectionPool(host='cdn-lfs-us-1.hf.co', port=443): Read timed out.\n",
      "Trying to resume download...\n",
      "Error while downloading from https://cdn-lfs-us-1.hf.co/repos/2b/ba/2bba274ddab7c3d74e51a70aa38841cffc67797619081eb47c9b29b09c0ea4a1/5f54e3323fc98caddba9626aa9771efd873c3cb9d63cc65b4619c2ccb6213e4e?response-content-disposition=inline%3B+filename*%3DUTF-8%27%27model.safetensors%3B+filename%3D%22model.safetensors%22%3B&Expires=1757436317&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTc1NzQzNjMxN319LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy11cy0xLmhmLmNvL3JlcG9zLzJiL2JhLzJiYmEyNzRkZGFiN2MzZDc0ZTUxYTcwYWEzODg0MWNmZmM2Nzc5NzYxOTA4MWViNDdjOWIyOWIwOWMwZWE0YTEvNWY1NGUzMzIzZmM5OGNhZGRiYTk2MjZhYTk3NzFlZmQ4NzNjM2NiOWQ2M2NjNjViNDYxOWMyY2NiNjIxM2U0ZT9yZXNwb25zZS1jb250ZW50LWRpc3Bvc2l0aW9uPSoifV19&Signature=MXVEa%7EIcqBX-Nt93d42WbQSQ3najLzw90HC8r6aJi%7ENSaFNdDYEeNm4xoaglEDYEZ64EgtrgNKtNBYv%7EfqUNDa8UyEwj%7EQaMSwQkPyy3-CYFWk169NcpaGXtj85l3XrQmKqtoW7Na4fTDD%7Ezy209CSp6In4FxX3bEYn-Utx8rQE7IQFF0DasPOEDTUG5tjfsWRcanhYCUTl9aVhd9C31kxqAJxOL8StZW0ztEwkvLWUn-fJXDyxId4zfu-pbfFAct6r2lMn3DkjFjXIXyFN5VqBIW-Nt6b0Czik30R6qsEDXovIDdFLoRirCZ6ZaCUaCblGfrVfVcjW2JT15QvcDyA__&Key-Pair-Id=K24J24Z295AEI9: HTTPSConnectionPool(host='cdn-lfs-us-1.hf.co', port=443): Read timed out.\n",
      "Trying to resume download...\n",
      "WARNING:huggingface_hub.file_download:Error while downloading from https://cdn-lfs-us-1.hf.co/repos/2b/ba/2bba274ddab7c3d74e51a70aa38841cffc67797619081eb47c9b29b09c0ea4a1/5f54e3323fc98caddba9626aa9771efd873c3cb9d63cc65b4619c2ccb6213e4e?response-content-disposition=inline%3B+filename*%3DUTF-8%27%27model.safetensors%3B+filename%3D%22model.safetensors%22%3B&Expires=1757436317&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTc1NzQzNjMxN319LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy11cy0xLmhmLmNvL3JlcG9zLzJiL2JhLzJiYmEyNzRkZGFiN2MzZDc0ZTUxYTcwYWEzODg0MWNmZmM2Nzc5NzYxOTA4MWViNDdjOWIyOWIwOWMwZWE0YTEvNWY1NGUzMzIzZmM5OGNhZGRiYTk2MjZhYTk3NzFlZmQ4NzNjM2NiOWQ2M2NjNjViNDYxOWMyY2NiNjIxM2U0ZT9yZXNwb25zZS1jb250ZW50LWRpc3Bvc2l0aW9uPSoifV19&Signature=MXVEa%7EIcqBX-Nt93d42WbQSQ3najLzw90HC8r6aJi%7ENSaFNdDYEeNm4xoaglEDYEZ64EgtrgNKtNBYv%7EfqUNDa8UyEwj%7EQaMSwQkPyy3-CYFWk169NcpaGXtj85l3XrQmKqtoW7Na4fTDD%7Ezy209CSp6In4FxX3bEYn-Utx8rQE7IQFF0DasPOEDTUG5tjfsWRcanhYCUTl9aVhd9C31kxqAJxOL8StZW0ztEwkvLWUn-fJXDyxId4zfu-pbfFAct6r2lMn3DkjFjXIXyFN5VqBIW-Nt6b0Czik30R6qsEDXovIDdFLoRirCZ6ZaCUaCblGfrVfVcjW2JT15QvcDyA__&Key-Pair-Id=K24J24Z295AEI9: HTTPSConnectionPool(host='cdn-lfs-us-1.hf.co', port=443): Read timed out.\n",
      "Trying to resume download...\n",
      "Error while downloading from https://cdn-lfs-us-1.hf.co/repos/2b/ba/2bba274ddab7c3d74e51a70aa38841cffc67797619081eb47c9b29b09c0ea4a1/5f54e3323fc98caddba9626aa9771efd873c3cb9d63cc65b4619c2ccb6213e4e?response-content-disposition=inline%3B+filename*%3DUTF-8%27%27model.safetensors%3B+filename%3D%22model.safetensors%22%3B&Expires=1757436317&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTc1NzQzNjMxN319LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy11cy0xLmhmLmNvL3JlcG9zLzJiL2JhLzJiYmEyNzRkZGFiN2MzZDc0ZTUxYTcwYWEzODg0MWNmZmM2Nzc5NzYxOTA4MWViNDdjOWIyOWIwOWMwZWE0YTEvNWY1NGUzMzIzZmM5OGNhZGRiYTk2MjZhYTk3NzFlZmQ4NzNjM2NiOWQ2M2NjNjViNDYxOWMyY2NiNjIxM2U0ZT9yZXNwb25zZS1jb250ZW50LWRpc3Bvc2l0aW9uPSoifV19&Signature=MXVEa%7EIcqBX-Nt93d42WbQSQ3najLzw90HC8r6aJi%7ENSaFNdDYEeNm4xoaglEDYEZ64EgtrgNKtNBYv%7EfqUNDa8UyEwj%7EQaMSwQkPyy3-CYFWk169NcpaGXtj85l3XrQmKqtoW7Na4fTDD%7Ezy209CSp6In4FxX3bEYn-Utx8rQE7IQFF0DasPOEDTUG5tjfsWRcanhYCUTl9aVhd9C31kxqAJxOL8StZW0ztEwkvLWUn-fJXDyxId4zfu-pbfFAct6r2lMn3DkjFjXIXyFN5VqBIW-Nt6b0Czik30R6qsEDXovIDdFLoRirCZ6ZaCUaCblGfrVfVcjW2JT15QvcDyA__&Key-Pair-Id=K24J24Z295AEI9: HTTPSConnectionPool(host='cdn-lfs-us-1.hf.co', port=443): Read timed out.\n",
      "Trying to resume download...\n",
      "WARNING:huggingface_hub.file_download:Error while downloading from https://cdn-lfs-us-1.hf.co/repos/2b/ba/2bba274ddab7c3d74e51a70aa38841cffc67797619081eb47c9b29b09c0ea4a1/5f54e3323fc98caddba9626aa9771efd873c3cb9d63cc65b4619c2ccb6213e4e?response-content-disposition=inline%3B+filename*%3DUTF-8%27%27model.safetensors%3B+filename%3D%22model.safetensors%22%3B&Expires=1757436317&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTc1NzQzNjMxN319LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy11cy0xLmhmLmNvL3JlcG9zLzJiL2JhLzJiYmEyNzRkZGFiN2MzZDc0ZTUxYTcwYWEzODg0MWNmZmM2Nzc5NzYxOTA4MWViNDdjOWIyOWIwOWMwZWE0YTEvNWY1NGUzMzIzZmM5OGNhZGRiYTk2MjZhYTk3NzFlZmQ4NzNjM2NiOWQ2M2NjNjViNDYxOWMyY2NiNjIxM2U0ZT9yZXNwb25zZS1jb250ZW50LWRpc3Bvc2l0aW9uPSoifV19&Signature=MXVEa%7EIcqBX-Nt93d42WbQSQ3najLzw90HC8r6aJi%7ENSaFNdDYEeNm4xoaglEDYEZ64EgtrgNKtNBYv%7EfqUNDa8UyEwj%7EQaMSwQkPyy3-CYFWk169NcpaGXtj85l3XrQmKqtoW7Na4fTDD%7Ezy209CSp6In4FxX3bEYn-Utx8rQE7IQFF0DasPOEDTUG5tjfsWRcanhYCUTl9aVhd9C31kxqAJxOL8StZW0ztEwkvLWUn-fJXDyxId4zfu-pbfFAct6r2lMn3DkjFjXIXyFN5VqBIW-Nt6b0Czik30R6qsEDXovIDdFLoRirCZ6ZaCUaCblGfrVfVcjW2JT15QvcDyA__&Key-Pair-Id=K24J24Z295AEI9: HTTPSConnectionPool(host='cdn-lfs-us-1.hf.co', port=443): Read timed out.\n",
      "Trying to resume download...\n",
      "Error while downloading from https://cdn-lfs-us-1.hf.co/repos/2b/ba/2bba274ddab7c3d74e51a70aa38841cffc67797619081eb47c9b29b09c0ea4a1/5f54e3323fc98caddba9626aa9771efd873c3cb9d63cc65b4619c2ccb6213e4e?response-content-disposition=inline%3B+filename*%3DUTF-8%27%27model.safetensors%3B+filename%3D%22model.safetensors%22%3B&Expires=1757436317&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTc1NzQzNjMxN319LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy11cy0xLmhmLmNvL3JlcG9zLzJiL2JhLzJiYmEyNzRkZGFiN2MzZDc0ZTUxYTcwYWEzODg0MWNmZmM2Nzc5NzYxOTA4MWViNDdjOWIyOWIwOWMwZWE0YTEvNWY1NGUzMzIzZmM5OGNhZGRiYTk2MjZhYTk3NzFlZmQ4NzNjM2NiOWQ2M2NjNjViNDYxOWMyY2NiNjIxM2U0ZT9yZXNwb25zZS1jb250ZW50LWRpc3Bvc2l0aW9uPSoifV19&Signature=MXVEa%7EIcqBX-Nt93d42WbQSQ3najLzw90HC8r6aJi%7ENSaFNdDYEeNm4xoaglEDYEZ64EgtrgNKtNBYv%7EfqUNDa8UyEwj%7EQaMSwQkPyy3-CYFWk169NcpaGXtj85l3XrQmKqtoW7Na4fTDD%7Ezy209CSp6In4FxX3bEYn-Utx8rQE7IQFF0DasPOEDTUG5tjfsWRcanhYCUTl9aVhd9C31kxqAJxOL8StZW0ztEwkvLWUn-fJXDyxId4zfu-pbfFAct6r2lMn3DkjFjXIXyFN5VqBIW-Nt6b0Czik30R6qsEDXovIDdFLoRirCZ6ZaCUaCblGfrVfVcjW2JT15QvcDyA__&Key-Pair-Id=K24J24Z295AEI9: HTTPSConnectionPool(host='cdn-lfs-us-1.hf.co', port=443): Read timed out.\n",
      "Trying to resume download...\n",
      "WARNING:huggingface_hub.file_download:Error while downloading from https://cdn-lfs-us-1.hf.co/repos/2b/ba/2bba274ddab7c3d74e51a70aa38841cffc67797619081eb47c9b29b09c0ea4a1/5f54e3323fc98caddba9626aa9771efd873c3cb9d63cc65b4619c2ccb6213e4e?response-content-disposition=inline%3B+filename*%3DUTF-8%27%27model.safetensors%3B+filename%3D%22model.safetensors%22%3B&Expires=1757436317&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTc1NzQzNjMxN319LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy11cy0xLmhmLmNvL3JlcG9zLzJiL2JhLzJiYmEyNzRkZGFiN2MzZDc0ZTUxYTcwYWEzODg0MWNmZmM2Nzc5NzYxOTA4MWViNDdjOWIyOWIwOWMwZWE0YTEvNWY1NGUzMzIzZmM5OGNhZGRiYTk2MjZhYTk3NzFlZmQ4NzNjM2NiOWQ2M2NjNjViNDYxOWMyY2NiNjIxM2U0ZT9yZXNwb25zZS1jb250ZW50LWRpc3Bvc2l0aW9uPSoifV19&Signature=MXVEa%7EIcqBX-Nt93d42WbQSQ3najLzw90HC8r6aJi%7ENSaFNdDYEeNm4xoaglEDYEZ64EgtrgNKtNBYv%7EfqUNDa8UyEwj%7EQaMSwQkPyy3-CYFWk169NcpaGXtj85l3XrQmKqtoW7Na4fTDD%7Ezy209CSp6In4FxX3bEYn-Utx8rQE7IQFF0DasPOEDTUG5tjfsWRcanhYCUTl9aVhd9C31kxqAJxOL8StZW0ztEwkvLWUn-fJXDyxId4zfu-pbfFAct6r2lMn3DkjFjXIXyFN5VqBIW-Nt6b0Czik30R6qsEDXovIDdFLoRirCZ6ZaCUaCblGfrVfVcjW2JT15QvcDyA__&Key-Pair-Id=K24J24Z295AEI9: HTTPSConnectionPool(host='cdn-lfs-us-1.hf.co', port=443): Read timed out.\n",
      "Trying to resume download...\n"
     ]
    },
    {
     "ename": "NotImplementedError",
     "evalue": "Cannot copy out of meta tensor; no data! Please use torch.nn.Module.to_empty() instead of torch.nn.Module.to() when moving module from meta to a different device.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNotImplementedError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 22\u001b[39m\n\u001b[32m     20\u001b[39m token = \u001b[33m\"\u001b[39m\u001b[33mhf_qTSnqkEnNXrfarZzITqFaLWrZksXKLxsye\u001b[39m\u001b[33m\"\u001b[39m  \u001b[38;5;66;03m# Replace with your token if needed\u001b[39;00m\n\u001b[32m     21\u001b[39m processor = AutoProcessor.from_pretrained(model_name, trust_remote_code=\u001b[38;5;28;01mTrue\u001b[39;00m, token=token, force_download=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m---> \u001b[39m\u001b[32m22\u001b[39m model = \u001b[43mAutoModel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m     23\u001b[39m model.eval()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\HP\\Desktop\\E-Vision-Projects\\Merchandiser_Products_Counter\\.venv\\Lib\\site-packages\\transformers\\models\\auto\\auto_factory.py:597\u001b[39m, in \u001b[36m_BaseAutoModelClass.from_pretrained\u001b[39m\u001b[34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[39m\n\u001b[32m    595\u001b[39m         model_class.register_for_auto_class(auto_class=\u001b[38;5;28mcls\u001b[39m)\n\u001b[32m    596\u001b[39m     model_class = add_generation_mixin_to_remote_model(model_class)\n\u001b[32m--> \u001b[39m\u001b[32m597\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmodel_class\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    598\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43mmodel_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mhub_kwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    599\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    600\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(config) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mcls\u001b[39m._model_mapping:\n\u001b[32m    601\u001b[39m     model_class = _get_model_class(config, \u001b[38;5;28mcls\u001b[39m._model_mapping)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\HP\\Desktop\\E-Vision-Projects\\Merchandiser_Products_Counter\\.venv\\Lib\\site-packages\\transformers\\modeling_utils.py:288\u001b[39m, in \u001b[36mrestore_default_dtype.<locals>._wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    286\u001b[39m old_dtype = torch.get_default_dtype()\n\u001b[32m    287\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m288\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    289\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    290\u001b[39m     torch.set_default_dtype(old_dtype)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\HP\\Desktop\\E-Vision-Projects\\Merchandiser_Products_Counter\\.venv\\Lib\\site-packages\\transformers\\modeling_utils.py:5103\u001b[39m, in \u001b[36mPreTrainedModel.from_pretrained\u001b[39m\u001b[34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, weights_only, *model_args, **kwargs)\u001b[39m\n\u001b[32m   5100\u001b[39m config = copy.deepcopy(config)  \u001b[38;5;66;03m# We do not want to modify the config inplace in from_pretrained.\u001b[39;00m\n\u001b[32m   5101\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m ContextManagers(model_init_context):\n\u001b[32m   5102\u001b[39m     \u001b[38;5;66;03m# Let's make sure we don't run the init function of buffer modules\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m5103\u001b[39m     model = \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43mmodel_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   5105\u001b[39m \u001b[38;5;66;03m# Make sure to tie the weights correctly\u001b[39;00m\n\u001b[32m   5106\u001b[39m model.tie_weights()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\.cache\\huggingface\\modules\\transformers_modules\\Marqo\\marqo-ecommerce-embeddings-L\\92e3b2606e34fe9a91b732229689e9fb9d422929\\marqo_fashionSigLIP.py:190\u001b[39m, in \u001b[36mMarqoFashionSigLIP.__init__\u001b[39m\u001b[34m(self, config)\u001b[39m\n\u001b[32m    188\u001b[39m \u001b[38;5;28msuper\u001b[39m().\u001b[34m__init__\u001b[39m(config)\n\u001b[32m    189\u001b[39m \u001b[38;5;28mself\u001b[39m.config = config\n\u001b[32m--> \u001b[39m\u001b[32m190\u001b[39m \u001b[38;5;28mself\u001b[39m.model = \u001b[43mcreate_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mopen_clip_model_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_dict\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m    191\u001b[39m \u001b[38;5;28mself\u001b[39m.model.eval()\n\u001b[32m    192\u001b[39m \u001b[38;5;28mself\u001b[39m.model.to(\u001b[38;5;28mself\u001b[39m.device)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\HP\\Desktop\\E-Vision-Projects\\Merchandiser_Products_Counter\\.venv\\Lib\\site-packages\\open_clip\\factory.py:502\u001b[39m, in \u001b[36mcreate_model\u001b[39m\u001b[34m(model_name, pretrained, load_weights, precision, device, jit, force_quick_gelu, force_custom_text, force_patch_dropout, force_image_size, force_preprocess_cfg, force_context_length, pretrained_image, pretrained_text, pretrained_image_path, pretrained_text_path, cache_dir, output_dict, require_pretrained, weights_only, **model_kwargs)\u001b[39m\n\u001b[32m    500\u001b[39m logging.info(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mInstantiating model architecture: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_class.\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m    501\u001b[39m model = model_class(**final_model_cfg, cast_dtype=cast_dtype)\n\u001b[32m--> \u001b[39m\u001b[32m502\u001b[39m \u001b[43m_set_model_device_and_precision\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprecision\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_timm_model\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    504\u001b[39m \u001b[38;5;66;03m# Load Full Pretrained CLIP Weights (if path exists)\u001b[39;00m\n\u001b[32m    505\u001b[39m pretrained_loaded = \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\HP\\Desktop\\E-Vision-Projects\\Merchandiser_Products_Counter\\.venv\\Lib\\site-packages\\open_clip\\factory.py:787\u001b[39m, in \u001b[36m_set_model_device_and_precision\u001b[39m\u001b[34m(model, device, precision, is_timm_model)\u001b[39m\n\u001b[32m    785\u001b[39m     model.to(device=device, dtype=dtype)\n\u001b[32m    786\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m787\u001b[39m     \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\HP\\Desktop\\E-Vision-Projects\\Merchandiser_Products_Counter\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1369\u001b[39m, in \u001b[36mModule.to\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1366\u001b[39m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1367\u001b[39m             \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1369\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\HP\\Desktop\\E-Vision-Projects\\Merchandiser_Products_Counter\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:928\u001b[39m, in \u001b[36mModule._apply\u001b[39m\u001b[34m(self, fn, recurse)\u001b[39m\n\u001b[32m    926\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[32m    927\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.children():\n\u001b[32m--> \u001b[39m\u001b[32m928\u001b[39m         \u001b[43mmodule\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    930\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[32m    931\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m torch._has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[32m    932\u001b[39m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[32m    933\u001b[39m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    938\u001b[39m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[32m    939\u001b[39m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\HP\\Desktop\\E-Vision-Projects\\Merchandiser_Products_Counter\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:928\u001b[39m, in \u001b[36mModule._apply\u001b[39m\u001b[34m(self, fn, recurse)\u001b[39m\n\u001b[32m    926\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[32m    927\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.children():\n\u001b[32m--> \u001b[39m\u001b[32m928\u001b[39m         \u001b[43mmodule\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    930\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[32m    931\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m torch._has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[32m    932\u001b[39m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[32m    933\u001b[39m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    938\u001b[39m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[32m    939\u001b[39m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "    \u001b[31m[... skipping similar frames: Module._apply at line 928 (1 times)]\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\HP\\Desktop\\E-Vision-Projects\\Merchandiser_Products_Counter\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:928\u001b[39m, in \u001b[36mModule._apply\u001b[39m\u001b[34m(self, fn, recurse)\u001b[39m\n\u001b[32m    926\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[32m    927\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.children():\n\u001b[32m--> \u001b[39m\u001b[32m928\u001b[39m         \u001b[43mmodule\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    930\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[32m    931\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m torch._has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[32m    932\u001b[39m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[32m    933\u001b[39m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    938\u001b[39m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[32m    939\u001b[39m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\HP\\Desktop\\E-Vision-Projects\\Merchandiser_Products_Counter\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:955\u001b[39m, in \u001b[36mModule._apply\u001b[39m\u001b[34m(self, fn, recurse)\u001b[39m\n\u001b[32m    951\u001b[39m \u001b[38;5;66;03m# Tensors stored in modules are graph leaves, and we don't want to\u001b[39;00m\n\u001b[32m    952\u001b[39m \u001b[38;5;66;03m# track autograd history of `param_applied`, so we have to use\u001b[39;00m\n\u001b[32m    953\u001b[39m \u001b[38;5;66;03m# `with torch.no_grad():`\u001b[39;00m\n\u001b[32m    954\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m torch.no_grad():\n\u001b[32m--> \u001b[39m\u001b[32m955\u001b[39m     param_applied = \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparam\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    956\u001b[39m p_should_use_set_data = compute_should_use_set_data(param, param_applied)\n\u001b[32m    958\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_subclasses\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mfake_tensor\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m FakeTensor\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\HP\\Desktop\\E-Vision-Projects\\Merchandiser_Products_Counter\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1362\u001b[39m, in \u001b[36mModule.to.<locals>.convert\u001b[39m\u001b[34m(t)\u001b[39m\n\u001b[32m   1360\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m   1361\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(e) == \u001b[33m\"\u001b[39m\u001b[33mCannot copy out of meta tensor; no data!\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m-> \u001b[39m\u001b[32m1362\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(\n\u001b[32m   1363\u001b[39m             \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m Please use torch.nn.Module.to_empty() instead of torch.nn.Module.to() \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1364\u001b[39m             \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mwhen moving module from meta to a different device.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1365\u001b[39m         ) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1366\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1367\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[31mNotImplementedError\u001b[39m: Cannot copy out of meta tensor; no data! Please use torch.nn.Module.to_empty() instead of torch.nn.Module.to() when moving module from meta to a different device."
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModel, AutoProcessor\n",
    "from PIL import Image\n",
    "import requests\n",
    "\n",
    "\n",
    "# Define device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Load model and processor\n",
    "model_name = \"Marqo/marqo-ecommerce-embeddings-L\"\n",
    "try:\n",
    "    processor = AutoProcessor.from_pretrained(model_name, trust_remote_code=True)\n",
    "    model = AutoModel.from_pretrained(model_name, trust_remote_code=True, device_map=device)\n",
    "    model.eval()\n",
    "except Exception as e:\n",
    "    print(f\"Error loading model: {e}\")\n",
    "    # Optional: Use Hugging Face token if authentication issue\n",
    "    token = \"\"  # Replace with your token if needed\n",
    "    processor = AutoProcessor.from_pretrained(model_name, trust_remote_code=True, token=token, force_download=True)\n",
    "    model = AutoModel.from_pretrained(model_name, trust_remote_code=True, token=token, device_map=device, force_download=True)\n",
    "    model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2f7b1418",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (256) must match the size of tensor b (196) at non-singleton dimension 1",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 15\u001b[39m\n\u001b[32m     12\u001b[39m image = \u001b[33m'\u001b[39m\u001b[33m../data/test_output/cropped_image_2.jpg\u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m     13\u001b[39m image = Image.open(image)\n\u001b[32m---> \u001b[39m\u001b[32m15\u001b[39m embedding = \u001b[43mget_embedding\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     17\u001b[39m embedding\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 5\u001b[39m, in \u001b[36mget_embedding\u001b[39m\u001b[34m(image)\u001b[39m\n\u001b[32m      3\u001b[39m \u001b[38;5;66;03m# processor.image_processor.do_rescale = False\u001b[39;00m\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m torch.no_grad():\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m     outputs = \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      6\u001b[39m     \u001b[38;5;28mprint\u001b[39m(outputs)\n\u001b[32m      7\u001b[39m     \u001b[38;5;66;03m# Use CLS token embedding\u001b[39;00m\n\u001b[32m      8\u001b[39m     \u001b[38;5;66;03m# embedding = outputs.last_hidden_state[:, 0, :].squeeze()\u001b[39;00m\n\u001b[32m      9\u001b[39m \u001b[38;5;66;03m# return embedding.numpy()\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\HP\\Desktop\\E-Vision-Projects\\Merchandiser_Products_Counter\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1773\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1771\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1772\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\HP\\Desktop\\E-Vision-Projects\\Merchandiser_Products_Counter\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1784\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1779\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1780\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1781\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1782\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1783\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1784\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1786\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1787\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\HP\\Desktop\\E-Vision-Projects\\Merchandiser_Products_Counter\\.venv\\Lib\\site-packages\\transformers\\utils\\generic.py:940\u001b[39m, in \u001b[36mcan_return_tuple.<locals>.wrapper\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    938\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m return_dict_passed \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    939\u001b[39m     return_dict = return_dict_passed\n\u001b[32m--> \u001b[39m\u001b[32m940\u001b[39m output = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    941\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m return_dict \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(output, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[32m    942\u001b[39m     output = output.to_tuple()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\HP\\Desktop\\E-Vision-Projects\\Merchandiser_Products_Counter\\.venv\\Lib\\site-packages\\transformers\\models\\siglip\\modeling_siglip.py:1046\u001b[39m, in \u001b[36mSiglipModel.forward\u001b[39m\u001b[34m(self, input_ids, pixel_values, attention_mask, position_ids, return_loss, output_attentions, output_hidden_states, interpolate_pos_encoding)\u001b[39m\n\u001b[32m   1041\u001b[39m output_attentions = output_attentions \u001b[38;5;28;01mif\u001b[39;00m output_attentions \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m.config.output_attentions\n\u001b[32m   1042\u001b[39m output_hidden_states = (\n\u001b[32m   1043\u001b[39m     output_hidden_states \u001b[38;5;28;01mif\u001b[39;00m output_hidden_states \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m.config.output_hidden_states\n\u001b[32m   1044\u001b[39m )\n\u001b[32m-> \u001b[39m\u001b[32m1046\u001b[39m vision_outputs: BaseModelOutputWithPooling = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mvision_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1047\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpixel_values\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpixel_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1048\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1049\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1050\u001b[39m \u001b[43m    \u001b[49m\u001b[43minterpolate_pos_encoding\u001b[49m\u001b[43m=\u001b[49m\u001b[43minterpolate_pos_encoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1051\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1053\u001b[39m text_outputs: BaseModelOutputWithPooling = \u001b[38;5;28mself\u001b[39m.text_model(\n\u001b[32m   1054\u001b[39m     input_ids=input_ids,\n\u001b[32m   1055\u001b[39m     attention_mask=attention_mask,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1058\u001b[39m     output_hidden_states=output_hidden_states,\n\u001b[32m   1059\u001b[39m )\n\u001b[32m   1061\u001b[39m image_embeds = vision_outputs.pooler_output\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\HP\\Desktop\\E-Vision-Projects\\Merchandiser_Products_Counter\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1773\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1771\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1772\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\HP\\Desktop\\E-Vision-Projects\\Merchandiser_Products_Counter\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1784\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1779\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1780\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1781\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1782\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1783\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1784\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1786\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1787\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\HP\\Desktop\\E-Vision-Projects\\Merchandiser_Products_Counter\\.venv\\Lib\\site-packages\\transformers\\utils\\generic.py:940\u001b[39m, in \u001b[36mcan_return_tuple.<locals>.wrapper\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    938\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m return_dict_passed \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    939\u001b[39m     return_dict = return_dict_passed\n\u001b[32m--> \u001b[39m\u001b[32m940\u001b[39m output = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    941\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m return_dict \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(output, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[32m    942\u001b[39m     output = output.to_tuple()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\HP\\Desktop\\E-Vision-Projects\\Merchandiser_Products_Counter\\.venv\\Lib\\site-packages\\transformers\\models\\siglip\\modeling_siglip.py:763\u001b[39m, in \u001b[36mSiglipVisionTransformer.forward\u001b[39m\u001b[34m(self, pixel_values, output_attentions, output_hidden_states, interpolate_pos_encoding)\u001b[39m\n\u001b[32m    758\u001b[39m output_attentions = output_attentions \u001b[38;5;28;01mif\u001b[39;00m output_attentions \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m.config.output_attentions\n\u001b[32m    759\u001b[39m output_hidden_states = (\n\u001b[32m    760\u001b[39m     output_hidden_states \u001b[38;5;28;01mif\u001b[39;00m output_hidden_states \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m.config.output_hidden_states\n\u001b[32m    761\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m763\u001b[39m hidden_states = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43membeddings\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpixel_values\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minterpolate_pos_encoding\u001b[49m\u001b[43m=\u001b[49m\u001b[43minterpolate_pos_encoding\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    765\u001b[39m encoder_outputs: BaseModelOutput = \u001b[38;5;28mself\u001b[39m.encoder(\n\u001b[32m    766\u001b[39m     inputs_embeds=hidden_states,\n\u001b[32m    767\u001b[39m     output_attentions=output_attentions,\n\u001b[32m    768\u001b[39m     output_hidden_states=output_hidden_states,\n\u001b[32m    769\u001b[39m )\n\u001b[32m    771\u001b[39m last_hidden_state = encoder_outputs.last_hidden_state\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\HP\\Desktop\\E-Vision-Projects\\Merchandiser_Products_Counter\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1773\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1771\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1772\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\HP\\Desktop\\E-Vision-Projects\\Merchandiser_Products_Counter\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1784\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1779\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1780\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1781\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1782\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1783\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1784\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1786\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1787\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\HP\\Desktop\\E-Vision-Projects\\Merchandiser_Products_Counter\\.venv\\Lib\\site-packages\\transformers\\models\\siglip\\modeling_siglip.py:276\u001b[39m, in \u001b[36mSiglipVisionEmbeddings.forward\u001b[39m\u001b[34m(self, pixel_values, interpolate_pos_encoding)\u001b[39m\n\u001b[32m    274\u001b[39m     embeddings = embeddings + \u001b[38;5;28mself\u001b[39m.interpolate_pos_encoding(embeddings, height, width)\n\u001b[32m    275\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m276\u001b[39m     embeddings = \u001b[43membeddings\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mposition_embedding\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    277\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m embeddings\n",
      "\u001b[31mRuntimeError\u001b[39m: The size of tensor a (256) must match the size of tensor b (196) at non-singleton dimension 1"
     ]
    }
   ],
   "source": [
    "def get_embedding(image):\n",
    "    inputs = processor(images=[image], padding='max_length',return_tensors=\"pt\")\n",
    "    # processor.image_processor.do_rescale = False\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "        print(outputs)\n",
    "        # Use CLS token embedding\n",
    "        # embedding = outputs.last_hidden_state[:, 0, :].squeeze()\n",
    "    # return embedding.numpy()\n",
    "    return outputs\n",
    "\n",
    "image = '../data/test_output/cropped_image_2.jpg'\n",
    "image = Image.open(image)\n",
    "\n",
    "embedding = get_embedding(image)\n",
    "\n",
    "embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98908793",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerEmbedding:\n",
    "    def __init__(self, model_name):\n",
    "        self.processor = AutoImageProcessor.from_pretrained(model_name)\n",
    "        self.model = AutoModel.from_pretrained(model_name)\n",
    "        \n",
    "    def get_embedding(self, image):\n",
    "        inputs = self.processor(images=image, return_tensors='pt')\n",
    "        with torch.no_grad():\n",
    "            outputs = self.model(**inputs)\n",
    "            embed = outputs['pooler_output'].squeeze()\n",
    "            # embed = embed.reshape(-1) if not use squeeze\n",
    "            embed = torch.nn.functional.normalize(embed, dim=0)\n",
    "        return embed.numpy()       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "998e2224",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FaissManager:\n",
    "    def __init__(self, embedding_dim):\n",
    "        self.index = faiss.IndexFlatIP(embedding_dim)\n",
    "        \n",
    "    def add_reference_images(self, embeddings, index_path):\n",
    "        vector = np.array(embeddings).astype(np.float32)\n",
    "        self.index.add(vector)\n",
    "        faiss.write_index(self.index, index_path)\n",
    "        return self.index\n",
    "    \n",
    "    def load_index(self, index_path):\n",
    "        self.index = faiss.read_index(index_path)\n",
    "        return self.index\n",
    "    \n",
    "    def find_similarity(self, query_embedding, index_path, top_k = 3):\n",
    "        self.index = self.load_index(index_path=index_path)\n",
    "        \n",
    "        query_features = query_embedding.astype(np.float32).reshape(1, -1)\n",
    "        \n",
    "        distances, indices = self.index.search(query_features, top_k)\n",
    "        \n",
    "        return distances, indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcb54b36",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DBEmbedding:\n",
    "    def __init__(self, model):\n",
    "        self.image_path = []\n",
    "        self.embedding_lis = []\n",
    "        self.model = model\n",
    "    def get_db_embedding(self, db_dir):\n",
    "        self.image_path = glob.glob(os.path.join(db_dir, '*/*.jpg'))\n",
    "        for img in self.image_path:\n",
    "            image = Image.open(img)\n",
    "            embedding = self.model.get_embedding(image)\n",
    "            self.embedding_lis.append(embedding)\n",
    "        return self.embedding_lis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11d39ceb",
   "metadata": {},
   "outputs": [],
   "source": [
    "DB_DIR = '../data/db'\n",
    "\n",
    "db_images = os.listdir(DB_DIR)\n",
    "db_image_paths = glob.glob(os.path.join(DB_DIR, '*/*.jpg'))\n",
    "emb_model = TransformerEmbedding(model_name)\n",
    "\n",
    "embedding_db = DBEmbedding(emb_model)\n",
    "\n",
    "embedding_lis = embedding_db.get_db_embedding(DB_DIR)\n",
    "\n",
    "embedding_dim = len(embedding_lis[0])\n",
    "embedding_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f815e62",
   "metadata": {},
   "outputs": [],
   "source": [
    "faiss_manager = FaissManager(embedding_dim=embedding_dim)\n",
    "\n",
    "INDEX_PATH = '../data/faiss_data/vector.index'\n",
    "\n",
    "faiss_manager.add_reference_images(embeddings=embedding_lis, index_path=INDEX_PATH)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Merchandiser_Products_Counter",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
