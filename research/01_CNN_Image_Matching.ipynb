{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4d03b58a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision import models\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import glob\n",
    "import random \n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import faiss\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "561db7d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "DB_DIR = '../data/db'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "65d2942a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Load pre-trained ResNet50 model\n",
    "# class FeatureExtractor(torch.nn.Module):\n",
    "#     def __init__(self):\n",
    "#         super(FeatureExtractor, self).__init__()\n",
    "#         model = models.resnet50(pretrained=True)\n",
    "#         self.feature_extractor = torch.nn.Sequential(*list(model.children())[:-1])  # Remove last layer\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         x = self.feature_extractor(x)\n",
    "#         return x.view(x.size(0), -1)  # Flatten output\n",
    "    \n",
    "# # Initialize model\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# feature_extractor = FeatureExtractor().to(device).eval()\n",
    "\n",
    "\n",
    "# # Define image transformation\n",
    "# transform = transforms.Compose([\n",
    "#     transforms.Resize((224, 224)),\n",
    "#     transforms.ToTensor(),\n",
    "# ])\n",
    "\n",
    "\n",
    "# def extract_cnn_features(image):\n",
    "#     \"\"\"Extracts feature embeddings using ResNet50.\"\"\"\n",
    "#     image = transform(image).unsqueeze(0).to(device)\n",
    "    \n",
    "#     with torch.no_grad():\n",
    "#         features = feature_extractor(image).cpu().numpy().flatten()\n",
    "    \n",
    "#     return features  # Return feature embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5791432a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNNFeatureExtractor(torch.nn.Module):\n",
    "    def __init__(self, device=None):\n",
    "        super(CNNFeatureExtractor, self).__init__()\n",
    "        self.model = models.resnet101(pretrained=True)\n",
    "        self.feature_model = torch.nn.Sequential(*list(self.model.children())[:-1])\n",
    "        self.transform = transforms.Compose([\n",
    "                                transforms.Resize((224, 224)),\n",
    "                                transforms.ToTensor(),\n",
    "                            ])\n",
    "        self.device =  device if device else torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.to(self.device)\n",
    "        self.eval()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.feature_model(x)\n",
    "        return x.view(x.size(0), -1)  # Flatten output\n",
    "        # return x\n",
    "        # return torch.flatten(x)\n",
    "    \n",
    "    def get_features(self, image):\n",
    "        image = self.transform(image).unsqueeze(0).to(self.device)\n",
    "        with torch.no_grad():\n",
    "            features = self.forward(image).cpu().numpy().flatten()\n",
    "            # features = self.forward(image).cpu().numpy()\n",
    "            \n",
    "        return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e8333214",
   "metadata": {},
   "outputs": [],
   "source": [
    "# embedding_model = CNNFeatureExtractor()\n",
    "# img = Image.open('../data/test_output/cropped_image_5.jpg')\n",
    "# embedding = embedding_model.get_features(img)\n",
    "# len(embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6844d1c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FaissManager:\n",
    "    def __init__(self, embedding_dim):\n",
    "        self.index = faiss.IndexFlatIP(embedding_dim)\n",
    "        \n",
    "    def add_reference_images(self, embeddings, index_path):\n",
    "        vector = np.array(embeddings).astype(np.float32)\n",
    "        self.index.add(vector)\n",
    "        faiss.write_index(self.index, index_path)\n",
    "        return self.index\n",
    "    \n",
    "    def load_index(self, index_path):\n",
    "        self.index = faiss.read_index(index_path)\n",
    "        return self.index\n",
    "    \n",
    "    def find_similarity(self, query_embedding, index_path, top_k = 3):\n",
    "        self.index = self.load_index(index_path=index_path)\n",
    "        \n",
    "        query_features = query_embedding.astype(np.float32).reshape(1, -1)\n",
    "        \n",
    "        distances, indices = self.index.search(query_features, top_k)\n",
    "        \n",
    "        return distances, indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "939db937",
   "metadata": {},
   "outputs": [],
   "source": [
    "db_images = os.listdir(DB_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4bc478a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "db_image_paths = glob.glob(os.path.join(DB_DIR, '*/*.jpg'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "645d054f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DBEmbedding:\n",
    "    def __init__(self, model):\n",
    "        self.image_path = []\n",
    "        self.embedding_lis = []\n",
    "        self.model = model\n",
    "    def get_db_embedding(self, db_dir):\n",
    "        self.image_path = glob.glob(os.path.join(db_dir, '*/*.jpg'))\n",
    "        for img in self.image_path:\n",
    "            image = Image.open(img)\n",
    "            embedding = self.model.get_features(image)\n",
    "            self.embedding_lis.append(embedding)\n",
    "        return self.embedding_lis          \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "82f8aa29",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\HP\\Desktop\\E-Vision-Projects\\Merchandiser_Products_Counter\\.venv\\Lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\HP\\Desktop\\E-Vision-Projects\\Merchandiser_Products_Counter\\.venv\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet101_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet101_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "model = CNNFeatureExtractor()\n",
    "\n",
    "embedding_list = DBEmbedding(model).get_db_embedding(DB_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c033b65e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2048"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(embedding_list[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6d0f7b86",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2048"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_dim = len(embedding_list[0])\n",
    "embedding_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2af60e16",
   "metadata": {},
   "outputs": [],
   "source": [
    "faiss_manager = FaissManager(embedding_dim=embedding_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fad3d74d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<faiss.swigfaiss_avx2.IndexFlatIP; proxy of <Swig Object of type 'faiss::IndexFlatIP *' at 0x000001B706388A20> >"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "INDEX_PATH = '../data/faiss_data/vector.index'\n",
    "\n",
    "faiss_manager.add_reference_images(embeddings=embedding_list, index_path=INDEX_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a9656997",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_image = '../data/test_output/cropped_image_3.jpg'\n",
    "query_embedding = model.get_features(Image.open(query_image))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "06e42a36",
   "metadata": {},
   "outputs": [],
   "source": [
    "distances, indices = faiss_manager.find_similarity(query_embedding=query_embedding, index_path=INDEX_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "763b2385",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[772.51025, 744.6521 , 733.35315]], dtype=float32),\n",
       " array([[ 9, 29, 37]]))"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "distances, indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb5edc98",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Merchandiser_Products_Counter",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
